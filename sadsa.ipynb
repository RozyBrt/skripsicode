{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import random\n",
    "\n",
    "# Membaca dataset\n",
    "df_selected = pd.read_csv('dataset-berlabel-aspek.csv')\n",
    "\n",
    "# Memuat stopwords\n",
    "def load_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return set(file.read().splitlines())\n",
    "stopwords2 = load_file('txt/stopwords-2.txt')\n",
    "\n",
    "# Mengahapus stopwords\n",
    "def preprocessing(text, stopwords):\n",
    "    text = [word for word in text.split() if word not in stopwords]\n",
    "    return ' '.join(text) \n",
    "\n",
    "# Menghapus stopwords dari kolom 'teks' dan menyimpannya dalam kolom baru 'teks-kmeans'\n",
    "df_selected['teks-kmeans'] = df_selected['teks'].apply(lambda x: preprocessing(x, stopwords2))\n",
    "\n",
    "centroid_sentences = {\n",
    "    'kompensasi': \"gaji kompensasi\",\n",
    "    'kepuasan_kerja': \"mental stres jam\",\n",
    "    'aktualisasi': \"berkembang kembang jabatan skill\",\n",
    "    'hubungan_kerja': \"hubungan jahat hubungan baik\"\n",
    "}\n",
    "# Menghitung posisi dalam DataFrame untuk setiap centroid\n",
    "num_rows = len(df_selected)\n",
    "posisi = {\n",
    "    int(num_rows * 0.25): centroid_sentences['kompensasi'],\n",
    "    int(num_rows * 0.50): centroid_sentences['kepuasan_kerja'],\n",
    "    int(num_rows * 0.75): centroid_sentences['aktualisasi'],\n",
    "    int(num_rows * 0.90): centroid_sentences['hubungan_kerja']\n",
    "}\n",
    "# Menyisipkan kalimat ke dalam DataFrame pada posisi yang ditentukan\n",
    "for pos, sentence in posisi.items():\n",
    "    df_selected.at[pos, 'teks-kmeans'] = sentence\n",
    "# Vektorisasi teks menggunakan TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df_selected['teks-kmeans'])  # Menggunakan kolom teks yang telah dibersihkan\n",
    "lokasi_centroid = X[list(posisi.keys())].toarray()\n",
    "\n",
    "# K-means clustering\n",
    "kmeans = KMeans(n_clusters=4, init=lokasi_centroid, n_init=10, random_state=0)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Menyimpan hasil klaster pada kolom baru 'skor-klaster-prediksi'\n",
    "df_selected['skor-klaster-prediksi'] = kmeans.labels_\n",
    "\n",
    "db_score = davies_bouldin_score(X.toarray(), kmeans.labels_)\n",
    "print(f\"Davies-Bouldin Score: {db_score:.2f}\")\n",
    "\n",
    "# Memisahkan klaster menjadi DataFrame yang berbeda dan menambahkan kolom 'label'\n",
    "clusters = [df_selected[df_selected['skor-klaster-prediksi'] == i][['teks-kmeans', 'label']].reset_index(drop=True) for i in range(4)]\n",
    "\n",
    "# Label untuk setiap klaster\n",
    "label_klaster = ['kompensasi', 'kepuasan kerja', 'aktualisasi', 'hubungan kerja']\n",
    "\n",
    "# Menampilkan dan menyimpan hasil\n",
    "for label, cleaned_data in zip(label_klaster, clusters):  # Menyesuaikan penggunaan zip\n",
    "    print(f\"Faktor {label.capitalize()}:\")\n",
    "    print(cleaned_data[['teks-kmeans', 'label']])\n",
    "\n",
    "    # Menyimpan data ke file\n",
    "    cleaned_data.to_csv(f'klaster/{label}.txt', sep='\\t', index=False,Â header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
